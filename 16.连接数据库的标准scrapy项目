 知识点
    连接数据库、创建scrapy项目、创建爬虫、Item容器、Item Pipeline、Models创建表、保存Item到数据库、Item


 创建scrapy项目
    在Code目录，使用scrapy提供的startproject命令创建一个scrapy项目，需要一个项目名称，我们要爬取实验楼的数据，所以将shiyanlou作为项目名
    scrapy startproject shiyanlou

    进入Code/shiyanlou目录，可以看到项目结构是这样的：
    shiyanlou/
        scrapy.cfg              # 部署配置文件
        shiyanlou/              # 项目名称
            __init__.py
            items.py            # 项目items定义在这里
            pipelines.py        # 项目pipelines定义在这里
            settings.py         # 项目配置文件
            spiders/            # 所有爬虫写在这个目录下面
                __init__.py

 创建爬虫
    scrapy genspider <name> <domain>
    name这个爬虫的名称，domain指定要爬取的网站
    进入第二个目录，运行下面的命令快速初始化一个爬虫模板：
    scrapy genspider courses shiyanlou.com
    scrapy会在spiders目录下新建一个courses.py文件，并且在文件中为我们初始化了代码结构
    里面包含的新的属性 allowed_domains 可以是一个列表或者字符串，包含这个爬虫可以爬取的域名。但这个属性是可选的，不需可以删除。


 Item
     爬虫的主要目标是从网页中提取结构化的信息，scrapy爬虫可以将爬取到的数据作为一个python dict返回，但是由于dict的无序性，所以它不适合存放结构性
    数据，scrapy推荐使用Item容器来存放爬取到的数据
    定义Item非常简单，只需要继承scrapy.Item类，将每个要爬取的数据声明为scrapy.Field()。
    如 name = scrapy.Field()


 Item Pipeline
    如果把scrapy想象成一个产品线，spider负责从网页上爬取数据，Item相当于一个包装盒，对爬取的数据进行标准化包装，然后把它们扔到Pipeline流水线中。

    主要在Pipeline对Item进行几项处理：
        验证爬取到的数据（检查Item是否有特定的field）
        检查数据是否重复
        存储到数据库

    在pipelines.py中
    process_item(self, item, spider) 方法，parse出来的item会被传入这里，这里编写的处理代码会作用到每一个item上面，
    这个方法必须返回一个item对象
    除了process_item还有两个常用的hooks方法，open_spider(爬虫开启的时候调用)和close_spider(爬虫关闭的时候调用)

 定义Model，创建表
    在shiyanlou/shiyanlou目录下创建models.py，在里面使用sqlalchemy语法定义courses表结构：
    from sqlalchemy import create_engine
    from sqlalchemy.ext.declarative import declarative_base
    from sqlalchemy import Column, Integer, String

    # 连接数据库
    engine = create_engine('mysql+mysqldb://root@localhost:3306/shiyanlou?charset=utf8')
    Base = declarative_base()

    # 创建Course类，映射到数据库
    class Course(Base):
        __tablename__ = 'courses'

        id = Column(Integer, primary_key=True)
        name = Column(String(64))
        description = Column(string(1024), index=True)
        type = Column(String(64))
        students = Column(Integer)

    # 应该是执行上述操作 创建表
    Base.metadata.create_all(engine)


 保存Item到数据库


     编写的Pipeline默认是关闭的，需要到settings.py取消注释，ITEM_PIPELINES= {'shiyanlou.pipelines.ShiyanlouPipeline': 300}
     ITEM_PIPELINES是一个字典，key代表pipeline的位置，值是一个数字,表示当开启多个pipeline时它的执行顺序，值小的先执行，通常设在100-1000之间

 运行
    在/shiyanlou/shiyanlou输入以下命令
    scrapy crawl courses


 Item过滤
    from scrapy.exceptions import DropItem

    对于不需要的item，raise DropItem异常
    raise DropItem('Course students less than 1000')




    等级
    In [28]: response.xpath('//div[@class="user-meta"]/span[2]/text()').re_first('[^\w]*(.+)')
    Out[28]: 'L1'

    名字
    In [31]: response.xpath('//div[@class="user-meta"]/span[1]/text()').re_first('[^\w]*(.+)')
    Out[31]: 'kevin_xk'

    时间
    In [19]: response.css('span.user-join-date ::text').re_first('[^\w]*(.{10})')
    Out[19]: '2014-06-19'

    状态 在职还是学生
    In [3]: response.xpath('//div[@class="user-status"]/span[1]/text()').re_first('[^\w]*(.+)')
    Out[3]: '在职'

    职位
    In [4]: response.xpath('//div[@class="user-status"]/span[2]/text()').re_first('[^\w]*(.+)')
    Out[4]: 'None'

    学习记录
    In [24]: response.css('div.tabs-left span::text').re_first('[^\w]*(\d+)')
    Out[24]: '12'


